---
title: "week05_JZ"
author: "Jinxiao Zhang"
date: "February 18, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rethinking)
library(tidyverse)
```

1. Consider the data(Wines2012) data table. These data are expert ratings
of 20 different French and American wines by 9 different French and American
judges. Your goal is to model score, the subjective rating assigned by
each judge to each wine. I recommend standardizing it.
In this first problem, consider only variation among judges and wines.
Construct index variables of judge and wine and then use these index variables
to construct a linear regression model. Justify your priors. You should
end up with 9 judge parameters and 20 wine parameters. Use ulam instead of
quap to build this model, and be sure to check the chains for convergence. If
you'd rather build the model directly in Stan or PyMC3, go ahead. I just want
you to use Hamiltonian Monte Carlo instead of quadratic approximation.
How do you interpret the variation among individual judges and individual
wines? Do you notice any patterns, just by plotting the differences?
Which judges gave the highest/lowest ratings? Which wines were rated worst/
best on average?

```{r Q1}
data(Wines2012)
d1 <- Wines2012


d2 <- list(
  score_std = standardize(d1$score),
  judge = as.integer(d1$judge),
  wine = as.integer(d1$wine)
)

m1 <- ulam(
  alist(
    score_std ~ dnorm(mu, sigma),
    mu <- bJ[judge] + bW[wine],
    bJ[judge] ~ dnorm(0, .5),
    bW[wine] ~ dnorm(0, .5),
    sigma ~ dexp(1)
  ), 
  data=d2, chains=4 , cores=4 , iter=1000
)
traceplot(m1)
precis(m1, depth = 2) %>% plot()


# Judge 8 Robert - lowest; judge 5 John - highest
# wine 18 I2 - lowest
```

2. Now consider three features of the wines and judges:
(1) flight: Whether the wine is red or white.
(2) wine.amer: Indicator variable for American wines.
(3) judge.amer: Indicator variable for American judges.
Use indicator or index variables to model the influence of these features on
the scores. Omit the individual judge and wine index variables from Problem
1. Do not include interaction effects yet. Again use ulam, justify your priors,
and be sure to check the chains. What do you conclude about the differences
among the wines and judges? Try to relate the results to the inferences in
Problem 1.

```{r Q2}
d3 <- list(
  score_std = standardize(d1$score),
  flight = ifelse(d1$flight=="red",1L,2L),
  WA = d1$wine.amer + 1L,
  JA = d1$judge.amer + 1L
)
str(d3)

m2 <- ulam(
  alist(
    score_std ~ dnorm(mu, sigma),
    mu <- bF[flight] + bWA[WA] + bJA[JA],
    bF[flight] ~ dnorm(0, .5),
    bWA[WA] ~ dnorm(0, .5),
    bJA[JA] ~ dnorm(0, .5),
    sigma ~ dexp(1)
  ), 
  data=d3, chains=4 , cores=4 , iter=1000
)
traceplot(m2)
precis(m2, depth = 2) %>% plot()

########## from the solution #######
#### index must be 1, 2, 3... (not from 0...)
d3b <- list(
  S = standardize(d1$score),
  wid = d1$wine.amer + 1L,
  jid = d1$judge.amer + 1L,
  fid = ifelse(d1$flight=="red",1L,2L)
)

m2b <- ulam(
  alist(
    S ~ dnorm( mu , sigma ),
    mu <- w[wid] + j[jid] + f[fid],
    w[wid] ~ dnorm( 0 , 0.5 ),
    j[wid] ~ dnorm( 0 , 0.5 ),
    f[wid] ~ dnorm( 0 , 0.5 ),
    sigma ~ dexp(1)
  ), data= d3b , chains=4 , cores=4 )

precis(m2b, 2) %>% plot()

pairs(m2b)
```


3. Now consider two-way interactions among the three features. You should
end up with three different interaction terms in your model. These will be
easier to build, if you use indicator variables. Again use ulam, justify your
priors, and be sure to check the chains. Explain what each interaction means.
Be sure to interpret the model's predictions on the outcome scale (mu, the
expected score), not on the scale of individual parameters. You can use link
to help with this, or just use your knowledge of the linear model instead.
What do you conclude about the features and the scores? Can you relate
the results of your model(s) to the individual judge and wine inferences from
Problem 1?
```{r Q3}
d4 <- list(
  S = standardize(d1$score),
  W = d1$wine.amer,
  J = d1$judge.amer,
  R = ifelse(d1$flight=="red",1L,0L)
)

m3 <- ulam(
  alist(
    S ~ dnorm(mu, sigma),
    mu <- a + bW*W + bJ*J + bR*R + 
      bWJ*W*J + bWR*W*R + bJR*J*R,
    a ~ dnorm(0, .2),
    c(bW, bJ, bR) ~ dnorm(0, .5),
    c(bWJ, bWR, bJR) ~ dnorm(0, .5),
    sigma ~ dexp(1)
  ), data = d4, chains = 4, cores = 4
)
traceplot(m3)
precis(m3) %>% plot()

```


```{r testing ulam}
data(rugged)
d <- rugged
d$log_gdp <- log(d$rgdppc_2000)
dd <- d[ complete.cases(d$rgdppc_2000) , ]
dd$log_gdp_std <- dd$log_gdp / mean(dd$log_gdp)
dd$rugged_std <- dd$rugged / max(dd$rugged)
dd$cid <- ifelse( dd$cont_africa==1 , 1 , 2 )


m8.5 <- quap(
              alist(
                log_gdp_std ~ dnorm( mu , sigma ) ,
                mu <- a[cid] + b[cid]*( rugged_std - 0.215 ) ,
                a[cid] ~ dnorm( 1 , 0.1 ) ,
                b[cid] ~ dnorm( 0 , 0.3 ) ,
                sigma ~ dexp( 1 )
              ) ,
              data=dd )
precis( m8.5 , depth=2 )


dat_slim <- list(
    log_gpd_std = dd$log_gdp_std,
    rugged_std = dd$rugged_std,
    cid = as.integer( dd$cid )
)
str(dat_slim)

# m9.1 <- ulam(
#   alist(
#     log_gdp_std ~ dnorm( mu , sigma ) ,
#     mu <- a[cid] + b[cid]*( rugged_std - 0.215 ) ,
#     a[cid] ~ dnorm( 1 , 0.1 ) ,
#     b[cid] ~ dnorm( 0 , 0.3 ) ,
#     sigma ~ dexp( 1 )
#   ) ,
#   data=dat_slim , chains=1 )

m9.1 <- ulam(
  alist(
    log_gdp_std ~ dnorm( mu , sigma ) ,
    mu <- a[cid] + b[cid]*( rugged_std - 0.215 ) ,
    a[cid] ~ dnorm( 1 , 0.1 ) ,
    b[cid] ~ dnorm( 0 , 0.3 ) ,
    sigma ~ dexp( 1 )
  ) ,
  data=dat_slim , chains=4 , cores=4 , iter=1000 )

precis(m9.1 , depth=2)

traceplot( m9.1 )
pairs(m9.1)
```
Error:
There were 19 divergent transitions after warmup. Increasing adapt_delta above 0.95 may help. See
http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmupThere were 188 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 10. See
http://mc-stan.org/misc/warnings.html#maximum-treedepth-exceededThere were 1 chains where the estimated Bayesian Fraction of Missing Information was low. See
http://mc-stan.org/misc/warnings.html#bfmi-lowExamine the pairs() plot to diagnose sampling problems

